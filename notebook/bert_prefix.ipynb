{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb0264c-5139-4cbb-bd4b-ca6db7462232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ec16cc-a1c6-46ba-ac8d-b515d6dee1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3274a197-0c0a-4592-aba7-8ce7eb5843f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def prefix_function(examples):\n",
    "    prefix_ids = 'Classify this text whether positive or negative :-> '\n",
    "\n",
    "    examples[\"prefix_ids\"] = len(examples['input_ids']) * [tokenizer(prefix_ids)['input_ids']]\n",
    "\n",
    "    return examples\n",
    "\n",
    "# First, apply tokenize_function to tokenized_datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Then, apply prefix_function to the tokenized datasets\n",
    "tokenized_datasets = tokenized_datasets.map(prefix_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f501ba-7e27-4aae-840e-f0b0ab7ced79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bertSKT import  PrefixForSequenceClassification, PromptForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config._name_or_path=model_name\n",
    "config.hidden_size=768\n",
    "config.num_hidden_layers=12\n",
    "config.n_head=12\n",
    "config.num_labels=2\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "config.hidden_dropout = 0.1\n",
    "config.model_type='bert'\n",
    "config.pooling=True\n",
    "config.tokenizer=tokenizer\n",
    "config.prefix='classify the text as positive or negative, text:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "300474a3-53b8-4213-b029-7438be85bed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PrefixForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.self.query.weight', 'bert.transformer.encoder.layer.9.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.self.value.weight', 'bert.transformer.encoder.layer.1.attention.self.query.weight', 'bert.transformer.encoder.layer.5.attention.self.value.bias', 'bert.transformer.encoder.layer.2.output.dense.weight', 'bert.transformer.encoder.layer.5.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.output.dense.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.self.value.weight', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.weight', 'bert.transformer.encoder.layer.1.attention.self.key.bias', 'bert.transformer.encoder.layer.5.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.5.attention.self.key.bias', 'bert.transformer.encoder.layer.7.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.query.bias', 'bert.transformer.encoder.layer.9.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.query.bias', 'bert.transformer.embeddings.LayerNorm.weight', 'bert.transformer.encoder.layer.5.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.attention.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.bias', 'bert.prefix_encoder.W.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.bias', 'bert.transformer.encoder.layer.0.attention.self.value.weight', 'bert.transformer.encoder.layer.2.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.self.key.bias', 'bert.prefix_encoder.projection.bias', 'bert.classifier.weight', 'bert.transformer.encoder.layer.4.attention.self.query.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.output.dense.weight', 'bert.transformer.encoder.layer.7.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.attention.self.value.weight', 'bert.transformer.encoder.layer.0.attention.self.query.weight', 'bert.transformer.encoder.layer.11.output.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.self.key.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.4.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.self.key.weight', 'bert.transformer.encoder.layer.6.attention.self.query.bias', 'bert.transformer.embeddings.token_type_embeddings.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.query.bias', 'bert.transformer.encoder.layer.5.attention.self.value.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.weight', 'bert.prefix_encoder.W.bias', 'bert.transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.7.output.dense.bias', 'bert.transformer.encoder.layer.9.output.dense.weight', 'bert.transformer.encoder.layer.1.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.self.query.bias', 'bert.transformer.encoder.layer.0.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.output.dense.weight', 'bert.transformer.encoder.layer.2.attention.self.query.bias', 'bert.transformer.encoder.layer.8.attention.self.key.bias', 'bert.transformer.encoder.layer.6.attention.self.key.weight', 'bert.transformer.encoder.layer.0.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.intermediate.dense.bias', 'bert.transformer.encoder.layer.5.attention.self.query.bias', 'bert.transformer.encoder.layer.8.output.dense.weight', 'bert.transformer.encoder.layer.8.attention.output.dense.weight', 'bert.transformer.encoder.layer.9.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.self.value.bias', 'bert.transformer.encoder.layer.8.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.query.weight', 'bert.transformer.encoder.layer.7.attention.self.value.bias', 'bert.transformer.encoder.layer.3.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.attention.output.dense.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.bias', 'bert.transformer.encoder.layer.3.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.dense.weight', 'bert.transformer.embeddings.word_embeddings.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.bias', 'bert.transformer.encoder.layer.3.output.dense.weight', 'bert.transformer.encoder.layer.4.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.output.LayerNorm.weight', 'bert.transformer.encoder.layer.11.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.attention.self.value.bias', 'bert.transformer.encoder.layer.3.attention.self.query.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.weight', 'bert.transformer.encoder.layer.6.intermediate.dense.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.bias', 'bert.transformer.encoder.layer.3.attention.self.key.bias', 'bert.transformer.encoder.layer.0.intermediate.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.attention.self.value.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.6.output.dense.weight', 'bert.transformer.encoder.layer.5.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.key.bias', 'bert.transformer.encoder.layer.0.output.dense.weight', 'bert.transformer.encoder.layer.8.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.11.attention.self.key.weight', 'bert.transformer.encoder.layer.2.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.3.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.self.key.weight', 'bert.transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.4.attention.output.dense.bias', 'bert.transformer.encoder.layer.1.attention.self.query.bias', 'bert.classifier.bias', 'bert.transformer.encoder.layer.11.intermediate.dense.weight', 'bert.transformer.encoder.layer.3.attention.output.dense.weight', 'bert.transformer.encoder.layer.0.attention.self.query.bias', 'bert.transformer.encoder.layer.3.attention.self.key.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.self.key.bias', 'bert.transformer.encoder.layer.0.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.self.key.weight', 'bert.transformer.encoder.layer.9.attention.output.dense.bias', 'bert.transformer.encoder.layer.11.output.LayerNorm.bias', 'bert.transformer.encoder.layer.2.attention.self.query.weight', 'bert.transformer.encoder.layer.6.output.dense.bias', 'bert.transformer.encoder.layer.10.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.attention.self.value.bias', 'bert.transformer.encoder.layer.1.attention.output.dense.bias', 'bert.transformer.encoder.layer.2.attention.self.key.weight', 'bert.transformer.encoder.layer.11.attention.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.10.output.dense.weight', 'bert.transformer.encoder.layer.5.output.dense.bias', 'bert.transformer.encoder.layer.3.attention.self.query.bias', 'bert.transformer.encoder.layer.6.output.LayerNorm.bias', 'bert.transformer.encoder.layer.5.attention.self.query.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.intermediate.dense.weight', 'bert.transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.1.output.dense.weight', 'bert.transformer.encoder.layer.1.output.LayerNorm.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.bias', 'bert.transformer.encoder.layer.8.attention.self.key.weight', 'bert.transformer.encoder.layer.7.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.output.LayerNorm.weight', 'bert.transformer.encoder.layer.2.attention.self.value.bias', 'bert.transformer.encoder.layer.9.attention.self.value.weight', 'bert.transformer.encoder.layer.10.attention.self.query.bias', 'bert.transformer.embeddings.position_embeddings.weight', 'bert.transformer.encoder.layer.8.output.dense.bias', 'bert.transformer.embeddings.LayerNorm.bias', 'bert.transformer.encoder.layer.7.intermediate.dense.weight', 'bert.transformer.encoder.layer.7.attention.output.dense.weight', 'bert.transformer.encoder.layer.10.attention.self.value.weight', 'bert.transformer.encoder.layer.11.intermediate.dense.bias', 'bert.transformer.encoder.layer.4.attention.self.query.bias', 'bert.transformer.encoder.layer.7.attention.self.value.weight', 'bert.transformer.pooler.dense.weight', 'bert.transformer.encoder.layer.8.attention.self.query.weight', 'bert.transformer.encoder.layer.9.intermediate.dense.weight', 'bert.transformer.encoder.layer.9.attention.self.value.bias', 'bert.transformer.encoder.layer.4.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.value.bias', 'bert.transformer.encoder.layer.10.attention.self.key.weight', 'bert.transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.10.attention.self.key.bias', 'bert.transformer.encoder.layer.4.attention.self.value.weight', 'bert.transformer.encoder.layer.3.output.dense.bias', 'bert.transformer.encoder.layer.0.attention.output.dense.bias', 'bert.transformer.encoder.layer.0.output.dense.bias', 'bert.transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.transformer.encoder.layer.8.intermediate.dense.bias', 'bert.transformer.encoder.layer.6.attention.self.value.weight', 'bert.prefix_encoder.projection.weight', 'bert.transformer.encoder.layer.2.intermediate.dense.bias', 'bert.transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.0.intermediate.dense.bias', 'bert.transformer.pooler.dense.bias', 'bert.transformer.encoder.layer.6.attention.self.query.weight', 'bert.transformer.encoder.layer.8.attention.self.value.weight', 'bert.transformer.encoder.layer.6.output.LayerNorm.weight', 'bert.transformer.encoder.layer.1.output.dense.bias', 'bert.transformer.encoder.layer.5.intermediate.dense.weight', 'bert.transformer.encoder.layer.2.attention.output.dense.weight', 'bert.transformer.encoder.layer.5.attention.output.dense.bias', 'bert.transformer.encoder.layer.4.output.dense.bias', 'bert.transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.transformer.encoder.layer.6.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix sequence length:  12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = PrefixForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dec78477-d1cd-4f89-8a91-08232e7e7d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 112359939\n",
      "Trainable Parameters: 2877699\n",
      "Percentage Trainable: 2.56114325587165003739%\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in the model\n",
    "total_parameters = model.num_parameters()\n",
    "\n",
    "# Total number of trainable parameters in the model\n",
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate the percentage of trainable parameters\n",
    "percentage_trainable = (trainable_parameters / total_parameters) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_parameters}\")\n",
    "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
    "print(f\"Percentage Trainable: {percentage_trainable:.20f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c02e11b6-6ffc-46dd-bd22-90a529ee5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "from sklearn.metrics import r2_score, accuracy_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    #print(\"logits\", logits)\n",
    "    #print(\"logits\", len(logits), len(logits[0]), len(logits[0][0]))\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    labels = p.label_ids\n",
    "    #print(\"labels\", labels)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\"acc\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./rfalcon_task_prefix',\n",
    "    num_train_epochs=10,\n",
    "    do_eval=True,\n",
    "    #learning_rate=0.001,\n",
    "    #bf16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    #optim=\"paged_adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92501c3b-02cf-494b-b716-a08e5bece03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3700' max='21050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3700/21050 16:58 < 1:19:38, 3.63 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.686411</td>\n",
       "      <td>0.521789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>0.628674</td>\n",
       "      <td>0.619266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.341113</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.310025</td>\n",
       "      <td>0.877294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.293771</td>\n",
       "      <td>0.881881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>0.287002</td>\n",
       "      <td>0.883028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.317800</td>\n",
       "      <td>0.278796</td>\n",
       "      <td>0.888761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.286029</td>\n",
       "      <td>0.877294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.273525</td>\n",
       "      <td>0.884174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.268625</td>\n",
       "      <td>0.892202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.264284</td>\n",
       "      <td>0.894495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.281273</td>\n",
       "      <td>0.876147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.880734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.260573</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.291165</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.253278</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.270100</td>\n",
       "      <td>0.259386</td>\n",
       "      <td>0.893349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.260933</td>\n",
       "      <td>0.894495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.263282</td>\n",
       "      <td>0.889908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.251269</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.250259</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.246618</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.245900</td>\n",
       "      <td>0.258884</td>\n",
       "      <td>0.893349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>0.905963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.240802</td>\n",
       "      <td>0.907110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.240491</td>\n",
       "      <td>0.904817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.230600</td>\n",
       "      <td>0.253402</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.274273</td>\n",
       "      <td>0.892202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.254747</td>\n",
       "      <td>0.901376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.243886</td>\n",
       "      <td>0.907110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.249423</td>\n",
       "      <td>0.907110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.252335</td>\n",
       "      <td>0.905963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.247575</td>\n",
       "      <td>0.911697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.242299</td>\n",
       "      <td>0.910550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.240998</td>\n",
       "      <td>0.905963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3700, training_loss=0.2867033221270587, metrics={'train_runtime': 1018.6974, 'train_samples_per_second': 661.129, 'train_steps_per_second': 20.664, 'total_flos': 8048872156568832.0, 'train_loss': 0.2867033221270587, 'epoch': 1.76})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics, #compute_metrics1,#compute_metrics_classification,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=7)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375dae3-61ef-476b-9a01-80eb421f47e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
