{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1510cc-2c33-48ce-b22e-39a7df77e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1010b1-ee7d-4e50-9deb-d853bd99214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5be269-962b-40e7-820c-4bd15233e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets scikit-learn sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6dbf99-0f8e-4e6e-be86-2218da3dffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"PyTorch b model.\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers import BloomConfig, BloomPreTrainedModel, BloomModel, AutoConfig, PreTrainedModel, AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutput, Seq2SeqLMOutput\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, transfromer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        self.transfromer=transfromer\n",
    "\n",
    "        word_embeddings = transfromer.word_embeddings\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        init_token_ids = tokenizer(config.text, return_tensors='pt')['input_ids']\n",
    "        print(\"Prefix sequence length: \", init_token_ids.shape[1])\n",
    "        tokenizer=None\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(init_token_ids.shape[1], config.hidden_size)\n",
    "\n",
    "        if config.transform==True:\n",
    "            self.transform = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        else:\n",
    "            self.transform=None\n",
    "     \n",
    "        init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)\n",
    "\n",
    "        word_embedding_weights = word_embeddings(init_token_ids).detach().clone()\n",
    "        word_embedding_weights = word_embedding_weights.to(torch.float32)\n",
    "        #print('word_embedding_weights', word_embedding_weights.shape)\n",
    "        #print('word_embedding_weights', word_embedding_weights.squeeze(0).shape)\n",
    "        self.embedding.weight = torch.nn.Parameter(word_embedding_weights.squeeze(0))  \n",
    "        global virtual_tokens \n",
    "        virtual_tokens = torch.arange(0, init_token_ids.shape[1])\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        device=None,\n",
    "        batch_size=None,\n",
    "\n",
    "    ):\n",
    "\n",
    "\n",
    "        inputs_embeds = self.embedding(virtual_tokens.to(device))\n",
    "        inputs_embeds=self.dropout(inputs_embeds)\n",
    "        outputs = self.transfromer(\n",
    "            inputs_embeds=inputs_embeds.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        )        \n",
    "        #print('working', outputs.past_key_values)\n",
    "        #print('working', projection)\n",
    "        past_key_values=outputs.past_key_values\n",
    "        if config.transform==True:\n",
    "        # Apply transformations\n",
    "            transformed_key_values = []\n",
    "            for layer in past_key_values:\n",
    "                key, value = layer\n",
    "                #print(key.shape, value.shape)\n",
    "                # Transpose, transform, and transpose back for key\n",
    "                transformed_key = self.transform(key.transpose(1, 2)).transpose(1, 2)\n",
    "                transformed_key=self.dropout(transformed_key)\n",
    "                # Transpose, transform, and transpose back for value\n",
    "                transformed_value = self.transform(value)\n",
    "                transformed_value = self.dropout(transformed_value)\n",
    "                transformed_key_values.append((transformed_key, transformed_value))\n",
    "\n",
    "            transformed_past_key_values = tuple(transformed_key_values)\n",
    "        \n",
    "            return  (transformed_past_key_values, inputs_embeds.shape[0])\n",
    "        else:\n",
    "            return  (past_key_values, inputs_embeds.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "class PrefixForSequenceClassification(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.transformer =  AutoModel.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        self.score = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.hidden_size // config.n_head\n",
    "        config.n_embd=self.n_embd\n",
    "\n",
    "        #print('self.prefix_ids', self.prefix_ids)\n",
    "        self.prompt_encoder = PrefixEncoder(config, self.transformer)\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        past_key_values, pre_length =  self.prompt_encoder(self.transformer.device, batch_size)\n",
    "        #print('prompts', prompts.shape)\n",
    "        #print('raw_tokens_embedding', raw_tokens_embedding)\n",
    "        #print('batch_size', batch_size, self.pre_seq_len)\n",
    "        #inputs_embeds = torch.cat((prompts, raw_tokens_embedding), dim=1)\n",
    "        prompt_attention_mask = torch.ones(batch_size, pre_length).to(self.transformer.device)\n",
    "        attention_mask = torch.cat((prompt_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        \n",
    "        hidden_states = self.dropout(outputs[0])\n",
    "\n",
    "        logits = self.score(hidden_states)\n",
    "        logits = torch.mean(logits, dim=1)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class PromptEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, word_embeddings):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        init_token_ids = tokenizer(config.text, return_tensors='pt')['input_ids']\n",
    "        print(\"Prompt sequence length: \", init_token_ids.shape[1])\n",
    "        #print(\"config.pre_seq_len, config.hidden_size\", config.pre_seq_len, config.hidden_size)\n",
    "        tokenizer=None\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(init_token_ids.shape[1], config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "        if config.transform==True:\n",
    "            self.transform = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        else:\n",
    "            self.transform=None\n",
    "            \n",
    "        init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)\n",
    "\n",
    "        word_embedding_weights = word_embeddings(init_token_ids).detach().clone()\n",
    "        word_embedding_weights = word_embedding_weights.to(torch.float32)\n",
    "        #print('word_embedding_weights', word_embedding_weights.shape)\n",
    "        #print('word_embedding_weights', word_embedding_weights.squeeze(0).shape)\n",
    "        self.embedding.weight = torch.nn.Parameter(word_embedding_weights.squeeze(0))  \n",
    "        global virtual_tokens \n",
    "        virtual_tokens = torch.arange(0, init_token_ids.shape[1])\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        device=None,\n",
    "        batch_size=None,\n",
    "\n",
    "    ):\n",
    "\n",
    "        projection = self.embedding(virtual_tokens.to(device))\n",
    "        projection=self.dropout(projection)\n",
    "        \n",
    "        if config.transform==True:\n",
    "            projection = self.transform(projection)\n",
    "            projection=self.dropout(projection)\n",
    "\n",
    "        return projection.repeat(batch_size, 1, 1)\n",
    "\n",
    "\n",
    "class PromptForSequenceClassification(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.transformer =  AutoModel.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        #prefix_ids = config.tokenizer(config.prefix, return_tensors='pt')['input_ids']\n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        self.score = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.hidden_size // config.n_head\n",
    "\n",
    "        #print('self.prefix_ids', self.prefix_ids)\n",
    "        self.prompt_encoder = PromptEncoder(config, self.transformer.word_embeddings )\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        raw_tokens_embedding = self.transformer.word_embeddings (input_ids)\n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        prompts =  self.prompt_encoder(self.transformer.device, batch_size)\n",
    "        #print('prompts', prompts.shape)\n",
    "        #print('raw_tokens_embedding', raw_tokens_embedding)\n",
    "        #print('batch_size', batch_size, self.pre_seq_len)\n",
    "        inputs_embeds = torch.cat((prompts, raw_tokens_embedding), dim=1)\n",
    "        prompt_attention_mask = torch.ones(batch_size, prompts.shape[1]).to(self.transformer.device)\n",
    "        attention_mask = torch.cat((prompt_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        outputs = self.transformer(\n",
    "            # input_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "            # past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        hidden_states = self.dropout(outputs[0])\n",
    "        logits = self.score(hidden_states)\n",
    "        logits = torch.mean(logits, dim=1)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52654071-04b0-4623-9948-1d249877efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fake_news_filipino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c5c9dc-d33e-4028-8893-b942400a4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694e8b2e-f0e9-4724-ac53-f2603d4d37b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'article'],\n",
       "        num_rows: 2564\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'article'],\n",
       "        num_rows: 642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2154b825-24b1-4efc-9491-5f50d1e1abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efec15cd-f9b0-4e8d-85e9-0cb9f456d00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5007803f02c342b1aafbdee5b36a9183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2564 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14af6d0dfb8b4b62b8be9e2cd22c18e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Llama 2 Tokenizer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", add_prefix_space=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete = ['article']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['article'], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d390576a-c7ba-43e2-ab0a-efdeb54990a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2564\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 642\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70c0b90-e696-4b64-b453-b12a9d2f656d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FalconConfig {\n",
       "  \"_name_or_path\": \"tiiuae/falcon-7b-instruct\",\n",
       "  \"alibi\": false,\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"architectures\": [\n",
       "    \"FalconForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"tiiuae/falcon-7b-instruct--configuration_falcon.FalconConfig\",\n",
       "    \"AutoModel\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconModel\",\n",
       "    \"AutoModelForCausalLM\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForCausalLM\",\n",
       "    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForQuestionAnswering\",\n",
       "    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForSequenceClassification\",\n",
       "    \"AutoModelForTokenClassification\": \"tiiuae/falcon-7b-instruct--modeling_falcon.FalconForTokenClassification\"\n",
       "  },\n",
       "  \"bias\": false,\n",
       "  \"bos_token_id\": 11,\n",
       "  \"eos_token_id\": 11,\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 4544,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"falcon\",\n",
       "  \"multi_query\": true,\n",
       "  \"new_decoder_architecture\": false,\n",
       "  \"num_attention_heads\": 71,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_kv_heads\": 71,\n",
       "  \"parallel_attn\": true,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 65024\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "model_name=\"tiiuae/falcon-7b-instruct\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5805e914-c233-434e-af47-5cfa056fa1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config._name_or_path=model_name\n",
    "config.hidden_size=4544\n",
    "config.num_hidden_layers=32\n",
    "config.n_head=71\n",
    "config.num_labels=2\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "config.hidden_dropout = 0.1\n",
    "config.transform=False\n",
    "config.text='classify the text as positive or negative, text:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3020a8-b7b7-4ff7-985c-e5b573dcfe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804588cd5a634fca8a6de88498a13901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sequence length:  11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd7eb57e04c4c0f975edfe205dc0ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptForSequenceClassification were not initialized from the model checkpoint at tiiuae/falcon-7b-instruct and are newly initialized: ['score.weight', 'prompt_encoder.embedding.weight', 'score.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = PromptForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0766c3dd-9068-415d-a203-7ab14de101e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6921779778\n",
      "Trainable Parameters: 59074\n",
      "Percentage Trainable: 0.00085345101830253578%\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in the model\n",
    "total_parameters = model.num_parameters()\n",
    "\n",
    "# Total number of trainable parameters in the model\n",
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate the percentage of trainable parameters\n",
    "percentage_trainable = (trainable_parameters / total_parameters) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_parameters}\")\n",
    "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
    "print(f\"Percentage Trainable: {percentage_trainable:.20f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97598ece-04d3-437f-a348-efec246b44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "from sklearn.metrics import r2_score, accuracy_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    #print(\"logits\", logits)\n",
    "    #print(\"logits\", len(logits), len(logits[0]), len(logits[0][0]))\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    labels = p.label_ids\n",
    "    #print(\"labels\", labels)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "\n",
    "\n",
    "    return {\"acc\": accuracy}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./rfalcon_task_prompt',\n",
    "    num_train_epochs=10,\n",
    "    do_eval=True,\n",
    "    #learning_rate=0.001,\n",
    "    #bf16=True,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    #optim=\"paged_adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9457939b-b951-4802-84b3-9cf119fad441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 642\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19045187-8b75-4cb2-bff4-2acc3c0a5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2570' max='2570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2570/2570 2:26:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.609614</td>\n",
       "      <td>0.672897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.517491</td>\n",
       "      <td>0.830218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>0.491138</td>\n",
       "      <td>0.766355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.452652</td>\n",
       "      <td>0.827103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.450700</td>\n",
       "      <td>0.436989</td>\n",
       "      <td>0.848910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.414144</td>\n",
       "      <td>0.858255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.404807</td>\n",
       "      <td>0.825545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.399259</td>\n",
       "      <td>0.825545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.380693</td>\n",
       "      <td>0.853583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.374257</td>\n",
       "      <td>0.864486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.373519</td>\n",
       "      <td>0.838006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.361656</td>\n",
       "      <td>0.872274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.357698</td>\n",
       "      <td>0.866044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.362546</td>\n",
       "      <td>0.838006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.352081</td>\n",
       "      <td>0.878505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.347127</td>\n",
       "      <td>0.869159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>0.344290</td>\n",
       "      <td>0.878505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.342784</td>\n",
       "      <td>0.870717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.341128</td>\n",
       "      <td>0.870717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.338499</td>\n",
       "      <td>0.878505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.337138</td>\n",
       "      <td>0.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.335898</td>\n",
       "      <td>0.883178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>0.335986</td>\n",
       "      <td>0.873832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.334519</td>\n",
       "      <td>0.883178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.334150</td>\n",
       "      <td>0.883178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./rfalcon_task_prompt/checkpoint-2500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2570, training_loss=0.4115899720544481, metrics={'train_runtime': 8794.6864, 'train_samples_per_second': 2.915, 'train_steps_per_second': 0.292, 'total_flos': 1.3048114584754176e+17, 'train_loss': 0.4115899720544481, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics, #compute_metrics1,#compute_metrics_classification,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=7)],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd9628-dc2e-4441-8738-6f28b5ed74f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1601' max='2570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1601/2570 1:30:03 < 54:34, 0.30 it/s, Epoch 6.23/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.328621</td>\n",
       "      <td>0.886293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.889408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.315313</td>\n",
       "      <td>0.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>0.312389</td>\n",
       "      <td>0.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.319523</td>\n",
       "      <td>0.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.309928</td>\n",
       "      <td>0.890966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.303190</td>\n",
       "      <td>0.887850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.307578</td>\n",
       "      <td>0.875389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.295829</td>\n",
       "      <td>0.903427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.294327</td>\n",
       "      <td>0.897196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.293950</td>\n",
       "      <td>0.894081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>0.904984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.287029</td>\n",
       "      <td>0.909657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.296192</td>\n",
       "      <td>0.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.283835</td>\n",
       "      <td>0.904984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/65 00:42 < 00:28, 0.90 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf5237-4f35-4592-beb7-28a80d52d99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
