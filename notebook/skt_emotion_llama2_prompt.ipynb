{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437f7499-a57b-48f2-8a39-9e7640c3577c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facad542-a38b-412d-a3a5-71bad8be82da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for dair-ai/emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dair-ai/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3027eff-b372-4ae7-b05b-a10eddbdb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5e89f-81a7-42fe-80bd-7df7e99c8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d0d859-2984-4f08-b9bd-0b3945340148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e587ac052974a3fb61b0df75752e7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a105a48720534ac98e136c9c9541ecfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e54561c8874f2fbefb64e76cb889a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d830e064e008488aa67c4bad7972433b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456f84e43d7451585bb752c9540ff9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name=\"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81695d9-0a11-4f19-906f-289e10a7e4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b51d608087e4d9ba1e11dc9966ce01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b55258153c6474cadd60207746b4fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44bc40c1d5f4ff1883450dce86f6796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete = ['text']\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return llama_tokenizer(examples['text'], truncation=True, max_length=350)\n",
    "\n",
    "llama_tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "llama_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "llama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa986df5-c30c-4ee9-8d88-6dbdb4b3a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd0d35b-c341-4444-8da9-556802371cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61f110f1c994f2a9b597c8ac77a51a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"togethercomputer/Llama-2-7B-32K-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoModelForCausalLM\": \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"\n",
       "  },\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 8.0,\n",
       "    \"type\": \"linear\"\n",
       "  },\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bcf834f-d0d1-482a-ae09-99ad4b1bdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "config._name_or_path=model_name\n",
    "config.hidden_size=4096\n",
    "config.num_hidden_layers=32\n",
    "config.n_head=32\n",
    "config.num_labels=6\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "config.hidden_dropout = 0.1\n",
    "config.transform=False\n",
    "config.text='Classify the joy or sadness or anger or fear or love or surprise from the text:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825c2621-ea3a-4dd1-8624-74204ef12385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe99fbd622648059a344fd2f2918bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aebdab8da2d46df9f024bc0f4e3e5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123beb27061f4a1b92a0a8cc66b22a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de09d2d88acf4ebaaf4861595d83bc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c1f9affeb34996aeff4fc61debe6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt sequence length:  20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7c4a3e55d54470a569d07fbcbba03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PromptForSequenceClassification were not initialized from the model checkpoint at togethercomputer/Llama-2-7B-32K-Instruct and are newly initialized: ['transformer.layers.31.mlp.gate_proj.weight', 'transformer.layers.0.mlp.up_proj.weight', 'transformer.layers.14.self_attn.k_proj.weight', 'transformer.layers.18.mlp.gate_proj.weight', 'transformer.layers.28.self_attn.v_proj.weight', 'transformer.layers.3.mlp.down_proj.weight', 'transformer.layers.5.self_attn.v_proj.weight', 'transformer.layers.3.mlp.up_proj.weight', 'transformer.layers.5.mlp.up_proj.weight', 'transformer.layers.9.self_attn.q_proj.weight', 'transformer.layers.16.mlp.up_proj.weight', 'transformer.layers.22.self_attn.q_proj.weight', 'transformer.layers.22.mlp.up_proj.weight', 'transformer.layers.13.self_attn.v_proj.weight', 'transformer.layers.20.self_attn.q_proj.weight', 'transformer.layers.23.input_layernorm.weight', 'transformer.layers.7.self_attn.v_proj.weight', 'transformer.layers.24.mlp.gate_proj.weight', 'transformer.layers.3.self_attn.o_proj.weight', 'transformer.layers.29.self_attn.q_proj.weight', 'transformer.layers.27.self_attn.o_proj.weight', 'transformer.layers.14.mlp.up_proj.weight', 'transformer.layers.28.self_attn.q_proj.weight', 'transformer.layers.11.mlp.gate_proj.weight', 'transformer.layers.25.self_attn.k_proj.weight', 'transformer.layers.25.post_attention_layernorm.weight', 'transformer.layers.26.self_attn.k_proj.weight', 'transformer.layers.13.mlp.gate_proj.weight', 'transformer.layers.19.input_layernorm.weight', 'transformer.layers.1.self_attn.k_proj.weight', 'prompt_encoder.embedding.weight', 'transformer.layers.17.post_attention_layernorm.weight', 'transformer.layers.10.self_attn.v_proj.weight', 'transformer.layers.7.mlp.up_proj.weight', 'transformer.layers.25.mlp.down_proj.weight', 'transformer.layers.24.self_attn.v_proj.weight', 'transformer.layers.8.post_attention_layernorm.weight', 'transformer.layers.10.mlp.up_proj.weight', 'transformer.layers.13.self_attn.k_proj.weight', 'transformer.layers.5.self_attn.o_proj.weight', 'transformer.layers.10.input_layernorm.weight', 'transformer.layers.16.self_attn.k_proj.weight', 'transformer.layers.26.post_attention_layernorm.weight', 'transformer.layers.4.mlp.down_proj.weight', 'transformer.layers.11.self_attn.v_proj.weight', 'transformer.layers.14.self_attn.q_proj.weight', 'transformer.layers.21.post_attention_layernorm.weight', 'transformer.layers.30.mlp.gate_proj.weight', 'transformer.layers.20.self_attn.v_proj.weight', 'transformer.layers.24.post_attention_layernorm.weight', 'transformer.layers.31.self_attn.q_proj.weight', 'transformer.layers.18.self_attn.o_proj.weight', 'transformer.layers.27.mlp.down_proj.weight', 'transformer.layers.21.self_attn.k_proj.weight', 'transformer.layers.22.mlp.gate_proj.weight', 'transformer.layers.20.input_layernorm.weight', 'transformer.layers.8.input_layernorm.weight', 'transformer.layers.12.self_attn.k_proj.weight', 'transformer.layers.11.post_attention_layernorm.weight', 'transformer.layers.22.self_attn.o_proj.weight', 'transformer.layers.24.mlp.up_proj.weight', 'transformer.layers.14.mlp.gate_proj.weight', 'score.bias', 'transformer.layers.21.input_layernorm.weight', 'transformer.layers.20.mlp.up_proj.weight', 'transformer.layers.27.self_attn.q_proj.weight', 'transformer.layers.13.self_attn.q_proj.weight', 'transformer.layers.31.self_attn.o_proj.weight', 'transformer.layers.19.self_attn.v_proj.weight', 'transformer.layers.9.input_layernorm.weight', 'transformer.layers.22.self_attn.v_proj.weight', 'transformer.layers.7.post_attention_layernorm.weight', 'transformer.layers.1.input_layernorm.weight', 'transformer.layers.23.self_attn.v_proj.weight', 'transformer.layers.7.self_attn.q_proj.weight', 'transformer.layers.16.self_attn.v_proj.weight', 'transformer.layers.21.self_attn.o_proj.weight', 'transformer.layers.5.self_attn.k_proj.weight', 'transformer.layers.4.mlp.up_proj.weight', 'transformer.layers.13.input_layernorm.weight', 'transformer.layers.14.mlp.down_proj.weight', 'transformer.layers.15.self_attn.q_proj.weight', 'transformer.layers.15.self_attn.k_proj.weight', 'transformer.layers.16.mlp.down_proj.weight', 'transformer.layers.18.self_attn.q_proj.weight', 'transformer.layers.8.self_attn.o_proj.weight', 'transformer.layers.1.mlp.up_proj.weight', 'transformer.layers.3.self_attn.k_proj.weight', 'transformer.layers.6.mlp.down_proj.weight', 'transformer.layers.7.input_layernorm.weight', 'transformer.layers.17.self_attn.k_proj.weight', 'transformer.layers.6.post_attention_layernorm.weight', 'transformer.layers.0.input_layernorm.weight', 'transformer.layers.8.mlp.up_proj.weight', 'transformer.layers.20.mlp.gate_proj.weight', 'transformer.layers.29.mlp.down_proj.weight', 'transformer.layers.7.mlp.down_proj.weight', 'transformer.layers.31.self_attn.v_proj.weight', 'transformer.layers.28.mlp.up_proj.weight', 'transformer.layers.11.self_attn.q_proj.weight', 'transformer.layers.27.mlp.up_proj.weight', 'transformer.layers.2.post_attention_layernorm.weight', 'transformer.layers.24.mlp.down_proj.weight', 'transformer.layers.15.input_layernorm.weight', 'transformer.layers.4.self_attn.v_proj.weight', 'transformer.layers.21.self_attn.q_proj.weight', 'transformer.layers.24.self_attn.o_proj.weight', 'transformer.layers.30.self_attn.q_proj.weight', 'transformer.layers.20.mlp.down_proj.weight', 'transformer.layers.11.mlp.up_proj.weight', 'transformer.layers.14.input_layernorm.weight', 'transformer.layers.29.self_attn.o_proj.weight', 'transformer.layers.15.self_attn.o_proj.weight', 'transformer.layers.10.self_attn.o_proj.weight', 'transformer.layers.15.mlp.down_proj.weight', 'transformer.layers.16.input_layernorm.weight', 'transformer.layers.17.self_attn.v_proj.weight', 'transformer.layers.30.self_attn.v_proj.weight', 'transformer.layers.9.self_attn.o_proj.weight', 'transformer.layers.18.post_attention_layernorm.weight', 'transformer.layers.0.self_attn.v_proj.weight', 'transformer.layers.16.self_attn.o_proj.weight', 'transformer.layers.14.self_attn.v_proj.weight', 'transformer.layers.21.mlp.gate_proj.weight', 'transformer.layers.26.self_attn.v_proj.weight', 'transformer.layers.8.self_attn.q_proj.weight', 'transformer.layers.28.mlp.down_proj.weight', 'transformer.layers.1.self_attn.v_proj.weight', 'transformer.layers.10.mlp.gate_proj.weight', 'transformer.layers.17.mlp.up_proj.weight', 'transformer.layers.21.mlp.up_proj.weight', 'transformer.layers.31.self_attn.k_proj.weight', 'transformer.layers.12.self_attn.v_proj.weight', 'transformer.layers.13.post_attention_layernorm.weight', 'transformer.layers.8.mlp.down_proj.weight', 'transformer.layers.14.self_attn.o_proj.weight', 'transformer.layers.27.input_layernorm.weight', 'transformer.layers.1.mlp.down_proj.weight', 'transformer.layers.3.mlp.gate_proj.weight', 'transformer.layers.15.self_attn.v_proj.weight', 'transformer.layers.27.self_attn.k_proj.weight', 'transformer.layers.30.post_attention_layernorm.weight', 'transformer.layers.4.self_attn.q_proj.weight', 'transformer.layers.12.mlp.up_proj.weight', 'transformer.layers.17.mlp.down_proj.weight', 'transformer.layers.22.post_attention_layernorm.weight', 'transformer.layers.24.self_attn.q_proj.weight', 'transformer.layers.24.input_layernorm.weight', 'transformer.layers.8.mlp.gate_proj.weight', 'transformer.layers.17.mlp.gate_proj.weight', 'transformer.layers.25.self_attn.q_proj.weight', 'transformer.layers.29.mlp.up_proj.weight', 'transformer.layers.26.self_attn.o_proj.weight', 'transformer.layers.28.self_attn.o_proj.weight', 'transformer.layers.30.self_attn.k_proj.weight', 'transformer.layers.7.mlp.gate_proj.weight', 'transformer.layers.5.mlp.down_proj.weight', 'transformer.layers.0.self_attn.q_proj.weight', 'transformer.layers.1.post_attention_layernorm.weight', 'transformer.layers.11.mlp.down_proj.weight', 'transformer.layers.28.post_attention_layernorm.weight', 'transformer.layers.30.self_attn.o_proj.weight', 'transformer.layers.15.mlp.gate_proj.weight', 'transformer.layers.12.mlp.down_proj.weight', 'transformer.layers.22.self_attn.k_proj.weight', 'transformer.layers.8.self_attn.k_proj.weight', 'transformer.layers.9.self_attn.v_proj.weight', 'transformer.layers.6.self_attn.k_proj.weight', 'transformer.layers.25.self_attn.v_proj.weight', 'transformer.layers.16.mlp.gate_proj.weight', 'score.weight', 'transformer.layers.6.self_attn.v_proj.weight', 'transformer.layers.25.input_layernorm.weight', 'transformer.layers.30.mlp.up_proj.weight', 'transformer.layers.1.self_attn.o_proj.weight', 'transformer.layers.18.self_attn.v_proj.weight', 'transformer.layers.28.mlp.gate_proj.weight', 'transformer.layers.9.mlp.up_proj.weight', 'transformer.layers.2.self_attn.q_proj.weight', 'transformer.layers.6.self_attn.q_proj.weight', 'transformer.layers.5.input_layernorm.weight', 'transformer.layers.31.post_attention_layernorm.weight', 'transformer.layers.31.input_layernorm.weight', 'transformer.layers.3.post_attention_layernorm.weight', 'transformer.layers.25.mlp.gate_proj.weight', 'transformer.layers.18.self_attn.k_proj.weight', 'transformer.layers.11.self_attn.k_proj.weight', 'transformer.layers.12.self_attn.o_proj.weight', 'transformer.layers.22.input_layernorm.weight', 'transformer.norm.weight', 'transformer.layers.8.self_attn.v_proj.weight', 'transformer.layers.13.mlp.up_proj.weight', 'transformer.layers.19.self_attn.k_proj.weight', 'transformer.layers.3.self_attn.q_proj.weight', 'transformer.layers.30.mlp.down_proj.weight', 'transformer.layers.11.input_layernorm.weight', 'transformer.layers.31.mlp.up_proj.weight', 'transformer.layers.5.post_attention_layernorm.weight', 'transformer.layers.12.self_attn.q_proj.weight', 'transformer.layers.16.self_attn.q_proj.weight', 'transformer.layers.18.mlp.up_proj.weight', 'transformer.layers.6.self_attn.o_proj.weight', 'transformer.layers.9.mlp.gate_proj.weight', 'transformer.layers.3.input_layernorm.weight', 'transformer.layers.17.input_layernorm.weight', 'transformer.layers.6.mlp.gate_proj.weight', 'transformer.layers.11.self_attn.o_proj.weight', 'transformer.layers.16.post_attention_layernorm.weight', 'transformer.layers.19.post_attention_layernorm.weight', 'transformer.layers.9.post_attention_layernorm.weight', 'transformer.layers.27.mlp.gate_proj.weight', 'transformer.layers.4.post_attention_layernorm.weight', 'transformer.layers.19.self_attn.o_proj.weight', 'transformer.layers.24.self_attn.k_proj.weight', 'transformer.layers.2.self_attn.o_proj.weight', 'transformer.layers.3.self_attn.v_proj.weight', 'transformer.layers.28.input_layernorm.weight', 'transformer.layers.10.self_attn.k_proj.weight', 'transformer.layers.17.self_attn.o_proj.weight', 'transformer.layers.26.mlp.down_proj.weight', 'transformer.layers.28.self_attn.k_proj.weight', 'transformer.layers.0.mlp.gate_proj.weight', 'transformer.layers.12.mlp.gate_proj.weight', 'transformer.layers.18.input_layernorm.weight', 'transformer.layers.15.post_attention_layernorm.weight', 'transformer.layers.26.mlp.gate_proj.weight', 'transformer.layers.29.self_attn.k_proj.weight', 'transformer.layers.10.mlp.down_proj.weight', 'transformer.layers.23.mlp.up_proj.weight', 'transformer.layers.21.self_attn.v_proj.weight', 'transformer.layers.26.self_attn.q_proj.weight', 'transformer.layers.23.post_attention_layernorm.weight', 'transformer.layers.23.self_attn.k_proj.weight', 'transformer.layers.25.mlp.up_proj.weight', 'transformer.layers.31.mlp.down_proj.weight', 'transformer.layers.7.self_attn.k_proj.weight', 'transformer.embed_tokens.weight', 'transformer.layers.2.mlp.gate_proj.weight', 'transformer.layers.4.self_attn.k_proj.weight', 'transformer.layers.4.input_layernorm.weight', 'transformer.layers.18.mlp.down_proj.weight', 'transformer.layers.21.mlp.down_proj.weight', 'transformer.layers.20.self_attn.k_proj.weight', 'transformer.layers.7.self_attn.o_proj.weight', 'transformer.layers.9.self_attn.k_proj.weight', 'transformer.layers.5.self_attn.q_proj.weight', 'transformer.layers.2.self_attn.v_proj.weight', 'transformer.layers.5.mlp.gate_proj.weight', 'transformer.layers.4.self_attn.o_proj.weight', 'transformer.layers.29.post_attention_layernorm.weight', 'transformer.layers.1.self_attn.q_proj.weight', 'transformer.layers.2.self_attn.k_proj.weight', 'transformer.layers.4.mlp.gate_proj.weight', 'transformer.layers.15.mlp.up_proj.weight', 'transformer.layers.13.mlp.down_proj.weight', 'transformer.layers.23.self_attn.o_proj.weight', 'transformer.layers.23.mlp.gate_proj.weight', 'transformer.layers.2.input_layernorm.weight', 'transformer.layers.17.self_attn.q_proj.weight', 'transformer.layers.29.self_attn.v_proj.weight', 'transformer.layers.14.post_attention_layernorm.weight', 'transformer.layers.29.input_layernorm.weight', 'transformer.layers.2.mlp.down_proj.weight', 'transformer.layers.27.self_attn.v_proj.weight', 'transformer.layers.0.self_attn.k_proj.weight', 'transformer.layers.26.mlp.up_proj.weight', 'transformer.layers.23.mlp.down_proj.weight', 'transformer.layers.13.self_attn.o_proj.weight', 'transformer.layers.19.self_attn.q_proj.weight', 'transformer.layers.29.mlp.gate_proj.weight', 'transformer.layers.2.mlp.up_proj.weight', 'transformer.layers.12.post_attention_layernorm.weight', 'transformer.layers.27.post_attention_layernorm.weight', 'transformer.layers.0.self_attn.o_proj.weight', 'transformer.layers.19.mlp.up_proj.weight', 'transformer.layers.6.mlp.up_proj.weight', 'transformer.layers.10.post_attention_layernorm.weight', 'transformer.layers.20.self_attn.o_proj.weight', 'transformer.layers.30.input_layernorm.weight', 'transformer.layers.19.mlp.down_proj.weight', 'transformer.layers.0.post_attention_layernorm.weight', 'transformer.layers.10.self_attn.q_proj.weight', 'transformer.layers.12.input_layernorm.weight', 'transformer.layers.22.mlp.down_proj.weight', 'transformer.layers.26.input_layernorm.weight', 'transformer.layers.9.mlp.down_proj.weight', 'transformer.layers.19.mlp.gate_proj.weight', 'transformer.layers.25.self_attn.o_proj.weight', 'transformer.layers.20.post_attention_layernorm.weight', 'transformer.layers.1.mlp.gate_proj.weight', 'transformer.layers.0.mlp.down_proj.weight', 'transformer.layers.23.self_attn.q_proj.weight', 'transformer.layers.6.input_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from llamaSKT import  PromptForSequenceClassification\n",
    "\n",
    "model = PromptForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "160ccc0b-127d-4616-9b96-6a1a2a5b65fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 6607450118\n",
      "Trainable Parameters: 106502\n",
      "Percentage Trainable: 0.00161184720426216319%\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in the model\n",
    "total_parameters = model.num_parameters()\n",
    "\n",
    "# Total number of trainable parameters in the model\n",
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate the percentage of trainable parameters\n",
    "percentage_trainable = (trainable_parameters / total_parameters) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_parameters}\")\n",
    "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
    "print(f\"Percentage Trainable: {percentage_trainable:.20f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758507d2-8e9b-4491-a970-385c1d374158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c4f8aad-713e-421f-a8d5-581b802597b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13335' max='13335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13335/13335 3:26:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.589200</td>\n",
       "      <td>1.529009</td>\n",
       "      <td>0.306554</td>\n",
       "      <td>0.189375</td>\n",
       "      <td>0.131407</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.513200</td>\n",
       "      <td>1.528721</td>\n",
       "      <td>0.166753</td>\n",
       "      <td>0.248930</td>\n",
       "      <td>0.189677</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.456700</td>\n",
       "      <td>1.435503</td>\n",
       "      <td>0.253892</td>\n",
       "      <td>0.283456</td>\n",
       "      <td>0.235081</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.417700</td>\n",
       "      <td>1.372740</td>\n",
       "      <td>0.524523</td>\n",
       "      <td>0.287132</td>\n",
       "      <td>0.240526</td>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.373700</td>\n",
       "      <td>1.311002</td>\n",
       "      <td>0.424610</td>\n",
       "      <td>0.358360</td>\n",
       "      <td>0.352733</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.297200</td>\n",
       "      <td>1.230281</td>\n",
       "      <td>0.587255</td>\n",
       "      <td>0.303663</td>\n",
       "      <td>0.262452</td>\n",
       "      <td>0.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.188100</td>\n",
       "      <td>1.183529</td>\n",
       "      <td>0.530947</td>\n",
       "      <td>0.329618</td>\n",
       "      <td>0.304548</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.161900</td>\n",
       "      <td>1.108806</td>\n",
       "      <td>0.680794</td>\n",
       "      <td>0.402476</td>\n",
       "      <td>0.413029</td>\n",
       "      <td>0.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.091700</td>\n",
       "      <td>1.063190</td>\n",
       "      <td>0.579622</td>\n",
       "      <td>0.481217</td>\n",
       "      <td>0.490156</td>\n",
       "      <td>0.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.018100</td>\n",
       "      <td>0.963616</td>\n",
       "      <td>0.675307</td>\n",
       "      <td>0.472644</td>\n",
       "      <td>0.481714</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.933066</td>\n",
       "      <td>0.746757</td>\n",
       "      <td>0.539803</td>\n",
       "      <td>0.562004</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.919900</td>\n",
       "      <td>0.877673</td>\n",
       "      <td>0.698376</td>\n",
       "      <td>0.540937</td>\n",
       "      <td>0.570029</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.850947</td>\n",
       "      <td>0.737130</td>\n",
       "      <td>0.534073</td>\n",
       "      <td>0.567489</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>0.714781</td>\n",
       "      <td>0.603941</td>\n",
       "      <td>0.633921</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>0.806128</td>\n",
       "      <td>0.759306</td>\n",
       "      <td>0.560512</td>\n",
       "      <td>0.591655</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.791600</td>\n",
       "      <td>0.769337</td>\n",
       "      <td>0.759007</td>\n",
       "      <td>0.604944</td>\n",
       "      <td>0.642448</td>\n",
       "      <td>0.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.715445</td>\n",
       "      <td>0.755062</td>\n",
       "      <td>0.648812</td>\n",
       "      <td>0.683927</td>\n",
       "      <td>0.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.740300</td>\n",
       "      <td>0.701894</td>\n",
       "      <td>0.764080</td>\n",
       "      <td>0.649355</td>\n",
       "      <td>0.686370</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.711300</td>\n",
       "      <td>0.703730</td>\n",
       "      <td>0.775414</td>\n",
       "      <td>0.641959</td>\n",
       "      <td>0.675882</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.678490</td>\n",
       "      <td>0.763675</td>\n",
       "      <td>0.671731</td>\n",
       "      <td>0.702556</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.699300</td>\n",
       "      <td>0.680640</td>\n",
       "      <td>0.771612</td>\n",
       "      <td>0.664854</td>\n",
       "      <td>0.698784</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.669682</td>\n",
       "      <td>0.789573</td>\n",
       "      <td>0.654259</td>\n",
       "      <td>0.694887</td>\n",
       "      <td>0.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.662799</td>\n",
       "      <td>0.793734</td>\n",
       "      <td>0.649361</td>\n",
       "      <td>0.694443</td>\n",
       "      <td>0.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.651033</td>\n",
       "      <td>0.799819</td>\n",
       "      <td>0.662495</td>\n",
       "      <td>0.706163</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.648698</td>\n",
       "      <td>0.794715</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.704990</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.649227</td>\n",
       "      <td>0.804594</td>\n",
       "      <td>0.668823</td>\n",
       "      <td>0.709437</td>\n",
       "      <td>0.792500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Could not locate the best model at ./r_task/checkpoint-12500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13335, training_loss=0.9647529696452738, metrics={'train_runtime': 12417.1072, 'train_samples_per_second': 6.443, 'train_steps_per_second': 1.074, 'total_flos': 1.2685136906133478e+17, 'train_loss': 0.9647529696452738, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./r_task',\n",
    "    #learning_rate=0.001,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    num_train_epochs=5,\n",
    " \n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=500,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=llama_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=llama_tokenized_datasets[\"validation\"],\n",
    "    tokenizer=llama_tokenizer,\n",
    "    data_collator=llama_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f8505e-816a-4b36-95db-bd8eee123cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13335' max='13335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13335/13335 3:29:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.682933</td>\n",
       "      <td>0.829779</td>\n",
       "      <td>0.627194</td>\n",
       "      <td>0.676155</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.629889</td>\n",
       "      <td>0.801554</td>\n",
       "      <td>0.659494</td>\n",
       "      <td>0.698544</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.647200</td>\n",
       "      <td>0.611979</td>\n",
       "      <td>0.806290</td>\n",
       "      <td>0.712392</td>\n",
       "      <td>0.747228</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.627952</td>\n",
       "      <td>0.813254</td>\n",
       "      <td>0.734793</td>\n",
       "      <td>0.761125</td>\n",
       "      <td>0.824500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.593918</td>\n",
       "      <td>0.804059</td>\n",
       "      <td>0.732576</td>\n",
       "      <td>0.755907</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.558774</td>\n",
       "      <td>0.814126</td>\n",
       "      <td>0.743324</td>\n",
       "      <td>0.771783</td>\n",
       "      <td>0.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.648206</td>\n",
       "      <td>0.837385</td>\n",
       "      <td>0.666490</td>\n",
       "      <td>0.718403</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.571160</td>\n",
       "      <td>0.835500</td>\n",
       "      <td>0.733272</td>\n",
       "      <td>0.771744</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.506570</td>\n",
       "      <td>0.829764</td>\n",
       "      <td>0.749666</td>\n",
       "      <td>0.777955</td>\n",
       "      <td>0.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.491959</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>0.745867</td>\n",
       "      <td>0.777274</td>\n",
       "      <td>0.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.523110</td>\n",
       "      <td>0.829673</td>\n",
       "      <td>0.779738</td>\n",
       "      <td>0.794395</td>\n",
       "      <td>0.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.480905</td>\n",
       "      <td>0.849308</td>\n",
       "      <td>0.773711</td>\n",
       "      <td>0.800335</td>\n",
       "      <td>0.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.518604</td>\n",
       "      <td>0.858551</td>\n",
       "      <td>0.743189</td>\n",
       "      <td>0.784117</td>\n",
       "      <td>0.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.519227</td>\n",
       "      <td>0.855174</td>\n",
       "      <td>0.763179</td>\n",
       "      <td>0.798069</td>\n",
       "      <td>0.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.468632</td>\n",
       "      <td>0.859867</td>\n",
       "      <td>0.771759</td>\n",
       "      <td>0.801565</td>\n",
       "      <td>0.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.478240</td>\n",
       "      <td>0.848932</td>\n",
       "      <td>0.769921</td>\n",
       "      <td>0.801350</td>\n",
       "      <td>0.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.441671</td>\n",
       "      <td>0.872760</td>\n",
       "      <td>0.772003</td>\n",
       "      <td>0.808362</td>\n",
       "      <td>0.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>0.863656</td>\n",
       "      <td>0.780149</td>\n",
       "      <td>0.812363</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.480287</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.781675</td>\n",
       "      <td>0.810987</td>\n",
       "      <td>0.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.446510</td>\n",
       "      <td>0.847336</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.822323</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.459500</td>\n",
       "      <td>0.437103</td>\n",
       "      <td>0.858072</td>\n",
       "      <td>0.802127</td>\n",
       "      <td>0.824723</td>\n",
       "      <td>0.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.445100</td>\n",
       "      <td>0.434481</td>\n",
       "      <td>0.860579</td>\n",
       "      <td>0.799294</td>\n",
       "      <td>0.823673</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.426461</td>\n",
       "      <td>0.861389</td>\n",
       "      <td>0.786546</td>\n",
       "      <td>0.816644</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.431718</td>\n",
       "      <td>0.863241</td>\n",
       "      <td>0.789704</td>\n",
       "      <td>0.819664</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.423479</td>\n",
       "      <td>0.862798</td>\n",
       "      <td>0.785713</td>\n",
       "      <td>0.816157</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.870967</td>\n",
       "      <td>0.789969</td>\n",
       "      <td>0.821980</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./r_task/checkpoint-12500/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13335, training_loss=0.50433715431262, metrics={'train_runtime': 12553.2084, 'train_samples_per_second': 6.373, 'train_steps_per_second': 1.062, 'total_flos': 1.2685136906133478e+17, 'train_loss': 0.50433715431262, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9729f32-3aa4-44dd-97bd-b66e23c0681f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
