{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437f7499-a57b-48f2-8a39-9e7640c3577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"PyTorch b model.\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutputWithPast,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "from transformers import BloomConfig, BloomPreTrainedModel, BloomModel, AutoConfig, PreTrainedModel, AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutput, Seq2SeqLMOutput\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PrefixEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, transfromer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        self.transfromer=transfromer\n",
    "\n",
    "        word_embeddings = transfromer.embed_tokens\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        init_token_ids = tokenizer(config.text, return_tensors='pt')['input_ids']\n",
    "        print(\"Prefix sequence length: \", init_token_ids.shape[1])\n",
    "        tokenizer=None\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(init_token_ids.shape[1], config.hidden_size)\n",
    "\n",
    "        if config.transform==True:\n",
    "            self.transform = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        else:\n",
    "            self.transform=None\n",
    "     \n",
    "        init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)\n",
    "\n",
    "        word_embedding_weights = word_embeddings(init_token_ids).detach().clone()\n",
    "        word_embedding_weights = word_embedding_weights.to(torch.float32)\n",
    "        #print('word_embedding_weights', word_embedding_weights.shape)\n",
    "        #print('word_embedding_weights', word_embedding_weights.squeeze(0).shape)\n",
    "        self.embedding.weight = torch.nn.Parameter(word_embedding_weights.squeeze(0))  \n",
    "        global virtual_tokens \n",
    "        virtual_tokens = torch.arange(0, init_token_ids.shape[1])\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        device=None,\n",
    "        batch_size=None,\n",
    "\n",
    "    ):\n",
    "\n",
    "\n",
    "        inputs_embeds = self.embedding(virtual_tokens.to(device))\n",
    "        inputs_embeds=self.dropout(inputs_embeds)\n",
    "        outputs = self.transfromer(\n",
    "            inputs_embeds=inputs_embeds.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        )        \n",
    "        #print('working', outputs.past_key_values)\n",
    "        #print('working', projection)\n",
    "        past_key_values=outputs.past_key_values\n",
    "        if config.transform==True:\n",
    "        # Apply transformations\n",
    "            transformed_key_values = []\n",
    "            for layer in past_key_values:\n",
    "                key, value = layer\n",
    "                #print(key.shape, value.shape)\n",
    "                # Transpose, transform, and transpose back for key\n",
    "                transformed_key = self.transform(key.transpose(1, 2)).transpose(1, 2)\n",
    "                transformed_key=self.dropout(transformed_key)\n",
    "                # Transpose, transform, and transpose back for value\n",
    "                transformed_value = self.transform(value)\n",
    "                transformed_value = self.dropout(transformed_value)\n",
    "                transformed_key_values.append((transformed_key, transformed_value))\n",
    "\n",
    "            transformed_past_key_values = tuple(transformed_key_values)\n",
    "        \n",
    "            return  (transformed_past_key_values, inputs_embeds.shape[0])\n",
    "        else:\n",
    "            return  (past_key_values, inputs_embeds.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "class PrefixForSequenceClassification(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.transformer =  AutoModel.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        self.score = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.hidden_size // config.n_head\n",
    "        config.n_embd=self.n_embd\n",
    "\n",
    "        #print('self.prefix_ids', self.prefix_ids)\n",
    "        self.prompt_encoder = PrefixEncoder(config, self.transformer)\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        past_key_values, pre_length =  self.prompt_encoder(self.transformer.device, batch_size)\n",
    "        #print('prompts', prompts.shape)\n",
    "        #print('raw_tokens_embedding', raw_tokens_embedding)\n",
    "        #print('batch_size', batch_size, self.pre_seq_len)\n",
    "        #inputs_embeds = torch.cat((prompts, raw_tokens_embedding), dim=1)\n",
    "        prompt_attention_mask = torch.ones(batch_size, pre_length).to(self.transformer.device)\n",
    "        attention_mask = torch.cat((prompt_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        outputs = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "        \n",
    "        hidden_states = self.dropout(outputs[0])\n",
    "\n",
    "        logits = self.score(hidden_states)\n",
    "        logits = torch.mean(logits, dim=1)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class PromptEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, word_embeddings):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        init_token_ids = tokenizer(config.text, return_tensors='pt')['input_ids']\n",
    "        print(\"Prompt sequence length: \", init_token_ids.shape[1])\n",
    "        #print(\"config.pre_seq_len, config.hidden_size\", config.pre_seq_len, config.hidden_size)\n",
    "        tokenizer=None\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(init_token_ids.shape[1], config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "\n",
    "        if config.transform==True:\n",
    "            self.transform = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        else:\n",
    "            self.transform=None\n",
    "            \n",
    "        init_token_ids = torch.LongTensor(init_token_ids).to(word_embeddings.weight.device)\n",
    "\n",
    "        word_embedding_weights = word_embeddings(init_token_ids).detach().clone()\n",
    "        word_embedding_weights = word_embedding_weights.to(torch.float32)\n",
    "        #print('word_embedding_weights', word_embedding_weights.shape)\n",
    "        #print('word_embedding_weights', word_embedding_weights.squeeze(0).shape)\n",
    "        self.embedding.weight = torch.nn.Parameter(word_embedding_weights.squeeze(0))  \n",
    "        global virtual_tokens \n",
    "        virtual_tokens = torch.arange(0, init_token_ids.shape[1])\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        device=None,\n",
    "        batch_size=None,\n",
    "\n",
    "    ):\n",
    "\n",
    "        projection = self.embedding(virtual_tokens.to(device))\n",
    "        projection=self.dropout(projection)\n",
    "        \n",
    "        if config.transform==True:\n",
    "            projection = self.transform(projection)\n",
    "            projection=self.dropout(projection)\n",
    "\n",
    "        return projection.repeat(batch_size, 1, 1)\n",
    "\n",
    "\n",
    "class PromptForSequenceClassification(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.transformer =  AutoModel.from_pretrained(config._name_or_path)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout)\n",
    "        #prefix_ids = config.tokenizer(config.prefix, return_tensors='pt')['input_ids']\n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        self.score = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.hidden_size // config.n_head\n",
    "\n",
    "        #print('self.prefix_ids', self.prefix_ids)\n",
    "        self.prompt_encoder = PromptEncoder(config, self.transformer.embed_tokens)\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        raw_tokens_embedding = self.transformer.embed_tokens(input_ids)\n",
    "        #print('prefix_ids', prefix_ids)\n",
    "        prompts =  self.prompt_encoder(self.transformer.device, batch_size)\n",
    "        #print('prompts', prompts.shape)\n",
    "        #print('raw_tokens_embedding', raw_tokens_embedding)\n",
    "        #print('batch_size', batch_size, self.pre_seq_len)\n",
    "        inputs_embeds = torch.cat((prompts, raw_tokens_embedding), dim=1)\n",
    "        prompt_attention_mask = torch.ones(batch_size, prompts.shape[1]).to(self.transformer.device)\n",
    "        attention_mask = torch.cat((prompt_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        outputs = self.transformer(\n",
    "            # input_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=return_dict,\n",
    "            # past_key_values=past_key_values,\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        hidden_states = self.dropout(outputs[0])\n",
    "        logits = self.score(hidden_states)\n",
    "        logits = torch.mean(logits, dim=1)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facad542-a38b-412d-a3a5-71bad8be82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "snli = load_dataset(\"snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fd7412-be6c-4e06-b19e-d859b0e7d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(snli[\"train\"])\n",
    "test = pd.DataFrame(snli[\"test\"])\n",
    "\n",
    "train = train[train[\"label\"]!=-1].reset_index(drop=True)\n",
    "\n",
    "test = test[test[\"label\"]!=-1].reset_index(drop=True)\n",
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d0d859-2984-4f08-b9bd-0b3945340148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81695d9-0a11-4f19-906f-289e10a7e4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2feb9af02804ea3bd77a9726f6949a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12107f2d6919414eaf19bd344ed71e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "# col_to_delete = ['idx']\n",
    "col_to_delete = ['premise','hypothesis']\n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa986df5-c30c-4ee9-8d88-6dbdb4b3a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd0d35b-c341-4444-8da9-556802371cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcf834f-d0d1-482a-ae09-99ad4b1bdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "config._name_or_path=model_name\n",
    "config.hidden_size=4096\n",
    "config.num_hidden_layers=32\n",
    "config.n_head=32\n",
    "config.num_labels=3\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "config.hidden_dropout = 0.1\n",
    "config.transform=False\n",
    "config.text='Recognize the textual entailment from the text:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825c2621-ea3a-4dd1-8624-74204ef12385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd305051760544f59093912f148450c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix sequence length:  14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596d5c2fca014e518b21d13121fd238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PrefixForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['prompt_encoder.transfromer.layers.5.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.17.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.o_proj.weight', 'transformer.layers.2.input_layernorm.weight', 'transformer.layers.21.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.up_proj.weight', 'transformer.layers.13.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.3.input_layernorm.weight', 'transformer.layers.11.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.gate_proj.weight', 'transformer.layers.6.mlp.gate_proj.weight', 'transformer.layers.25.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.k_proj.weight', 'transformer.layers.17.mlp.down_proj.weight', 'transformer.layers.12.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.27.input_layernorm.weight', 'prompt_encoder.transfromer.layers.25.self_attn.o_proj.weight', 'transformer.layers.12.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.23.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.k_proj.weight', 'transformer.layers.6.self_attn.k_proj.weight', 'transformer.layers.28.post_attention_layernorm.weight', 'transformer.layers.15.input_layernorm.weight', 'prompt_encoder.transfromer.layers.15.input_layernorm.weight', 'prompt_encoder.transfromer.layers.24.self_attn.q_proj.weight', 'transformer.layers.20.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.o_proj.weight', 'transformer.layers.29.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.q_proj.weight', 'transformer.layers.7.mlp.down_proj.weight', 'transformer.layers.2.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.23.input_layernorm.weight', 'prompt_encoder.transfromer.layers.31.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.29.self_attn.q_proj.weight', 'transformer.layers.11.self_attn.k_proj.weight', 'transformer.layers.25.mlp.down_proj.weight', 'transformer.layers.27.mlp.gate_proj.weight', 'transformer.layers.13.self_attn.k_proj.weight', 'transformer.layers.20.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.q_proj.weight', 'transformer.layers.28.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.14.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.0.post_attention_layernorm.weight', 'transformer.layers.14.self_attn.q_proj.weight', 'transformer.layers.17.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.21.post_attention_layernorm.weight', 'transformer.layers.6.mlp.up_proj.weight', 'prompt_encoder.transfromer.embed_tokens.weight', 'prompt_encoder.transfromer.layers.29.self_attn.k_proj.weight', 'transformer.layers.1.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.31.input_layernorm.weight', 'prompt_encoder.transfromer.layers.20.mlp.up_proj.weight', 'transformer.layers.7.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.v_proj.weight', 'transformer.layers.1.self_attn.v_proj.weight', 'transformer.layers.3.input_layernorm.weight', 'transformer.layers.16.post_attention_layernorm.weight', 'transformer.layers.4.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.1.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.12.self_attn.v_proj.weight', 'score.bias', 'prompt_encoder.transfromer.layers.14.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.10.mlp.down_proj.weight', 'transformer.layers.19.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.17.input_layernorm.weight', 'prompt_encoder.transfromer.layers.17.self_attn.q_proj.weight', 'transformer.layers.20.self_attn.k_proj.weight', 'transformer.layers.10.input_layernorm.weight', 'transformer.layers.0.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.up_proj.weight', 'transformer.layers.4.self_attn.q_proj.weight', 'transformer.layers.5.post_attention_layernorm.weight', 'transformer.layers.15.mlp.down_proj.weight', 'transformer.layers.14.self_attn.k_proj.weight', 'transformer.layers.11.input_layernorm.weight', 'transformer.layers.1.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.2.input_layernorm.weight', 'transformer.layers.7.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.10.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.26.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.3.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.23.mlp.up_proj.weight', 'transformer.layers.11.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.28.input_layernorm.weight', 'prompt_encoder.transfromer.layers.7.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.mlp.up_proj.weight', 'transformer.layers.0.mlp.down_proj.weight', 'transformer.layers.30.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.7.mlp.up_proj.weight', 'transformer.layers.25.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.16.mlp.up_proj.weight', 'transformer.layers.13.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.0.mlp.up_proj.weight', 'transformer.layers.4.self_attn.o_proj.weight', 'transformer.layers.11.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.28.mlp.gate_proj.weight', 'transformer.layers.2.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.4.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.30.post_attention_layernorm.weight', 'score.weight', 'transformer.layers.23.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.k_proj.weight', 'transformer.layers.15.self_attn.k_proj.weight', 'transformer.layers.6.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.6.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.25.input_layernorm.weight', 'transformer.layers.3.self_attn.q_proj.weight', 'transformer.layers.17.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.31.mlp.down_proj.weight', 'transformer.layers.18.self_attn.v_proj.weight', 'transformer.layers.27.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.o_proj.weight', 'transformer.layers.8.self_attn.v_proj.weight', 'transformer.layers.1.post_attention_layernorm.weight', 'transformer.layers.27.input_layernorm.weight', 'transformer.layers.29.mlp.gate_proj.weight', 'transformer.layers.31.mlp.up_proj.weight', 'transformer.layers.24.self_attn.o_proj.weight', 'transformer.layers.22.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.2.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.23.self_attn.v_proj.weight', 'transformer.layers.16.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.up_proj.weight', 'transformer.layers.1.self_attn.k_proj.weight', 'transformer.layers.7.mlp.gate_proj.weight', 'transformer.norm.weight', 'prompt_encoder.transfromer.layers.22.mlp.up_proj.weight', 'transformer.layers.27.self_attn.k_proj.weight', 'transformer.layers.28.self_attn.q_proj.weight', 'transformer.layers.14.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.18.self_attn.v_proj.weight', 'transformer.layers.9.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.16.mlp.gate_proj.weight', 'transformer.layers.21.input_layernorm.weight', 'transformer.layers.26.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.10.input_layernorm.weight', 'transformer.layers.0.self_attn.o_proj.weight', 'transformer.layers.0.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.9.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.24.self_attn.v_proj.weight', 'transformer.layers.12.input_layernorm.weight', 'transformer.layers.20.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.o_proj.weight', 'transformer.layers.21.self_attn.v_proj.weight', 'transformer.layers.23.self_attn.q_proj.weight', 'transformer.layers.20.self_attn.q_proj.weight', 'transformer.layers.29.input_layernorm.weight', 'prompt_encoder.transfromer.layers.11.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.v_proj.weight', 'transformer.layers.8.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.9.self_attn.o_proj.weight', 'transformer.layers.9.post_attention_layernorm.weight', 'transformer.layers.30.mlp.up_proj.weight', 'transformer.layers.16.mlp.down_proj.weight', 'transformer.layers.13.input_layernorm.weight', 'prompt_encoder.transfromer.layers.11.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.v_proj.weight', 'transformer.layers.24.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.post_attention_layernorm.weight', 'transformer.layers.25.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.0.input_layernorm.weight', 'transformer.layers.15.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.2.post_attention_layernorm.weight', 'transformer.layers.22.self_attn.v_proj.weight', 'transformer.layers.5.self_attn.q_proj.weight', 'transformer.layers.24.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.12.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.q_proj.weight', 'transformer.layers.26.input_layernorm.weight', 'prompt_encoder.transfromer.layers.15.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.23.post_attention_layernorm.weight', 'transformer.layers.31.mlp.gate_proj.weight', 'transformer.layers.25.post_attention_layernorm.weight', 'transformer.layers.25.self_attn.o_proj.weight', 'transformer.layers.10.self_attn.o_proj.weight', 'transformer.layers.26.self_attn.k_proj.weight', 'transformer.layers.5.self_attn.o_proj.weight', 'transformer.layers.19.self_attn.k_proj.weight', 'transformer.layers.6.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.1.post_attention_layernorm.weight', 'transformer.layers.31.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.26.mlp.down_proj.weight', 'transformer.layers.15.mlp.up_proj.weight', 'transformer.layers.3.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.29.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.12.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.k_proj.weight', 'transformer.layers.19.self_attn.q_proj.weight', 'transformer.layers.1.self_attn.q_proj.weight', 'transformer.layers.8.mlp.down_proj.weight', 'transformer.layers.0.input_layernorm.weight', 'prompt_encoder.transfromer.layers.14.input_layernorm.weight', 'prompt_encoder.transfromer.layers.22.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.8.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.9.mlp.up_proj.weight', 'transformer.layers.2.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.gate_proj.weight', 'transformer.layers.15.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.9.self_attn.k_proj.weight', 'transformer.layers.3.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.v_proj.weight', 'transformer.layers.28.self_attn.v_proj.weight', 'transformer.layers.21.mlp.gate_proj.weight', 'transformer.layers.7.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.o_proj.weight', 'transformer.layers.5.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.7.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.21.input_layernorm.weight', 'prompt_encoder.transfromer.layers.1.self_attn.k_proj.weight', 'transformer.layers.8.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.23.self_attn.q_proj.weight', 'transformer.layers.29.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.15.mlp.up_proj.weight', 'transformer.layers.18.mlp.down_proj.weight', 'transformer.layers.10.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.16.post_attention_layernorm.weight', 'transformer.layers.17.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.30.input_layernorm.weight', 'prompt_encoder.transfromer.layers.9.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.13.post_attention_layernorm.weight', 'transformer.layers.13.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.q_proj.weight', 'transformer.layers.17.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.10.mlp.gate_proj.weight', 'transformer.layers.0.mlp.up_proj.weight', 'transformer.layers.2.self_attn.k_proj.weight', 'transformer.layers.19.mlp.down_proj.weight', 'transformer.layers.3.mlp.down_proj.weight', 'transformer.layers.7.self_attn.v_proj.weight', 'transformer.layers.8.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.29.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.k_proj.weight', 'transformer.layers.21.mlp.up_proj.weight', 'transformer.layers.24.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.8.input_layernorm.weight', 'transformer.layers.18.mlp.up_proj.weight', 'transformer.layers.3.self_attn.k_proj.weight', 'transformer.layers.21.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.24.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.17.mlp.down_proj.weight', 'transformer.layers.12.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.20.mlp.down_proj.weight', 'transformer.layers.10.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.5.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.30.mlp.up_proj.weight', 'transformer.layers.28.input_layernorm.weight', 'transformer.layers.16.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.18.post_attention_layernorm.weight', 'transformer.layers.17.input_layernorm.weight', 'prompt_encoder.transfromer.layers.5.input_layernorm.weight', 'prompt_encoder.transfromer.layers.31.post_attention_layernorm.weight', 'transformer.layers.23.input_layernorm.weight', 'prompt_encoder.transfromer.layers.29.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.k_proj.weight', 'transformer.layers.28.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.17.mlp.gate_proj.weight', 'transformer.layers.8.input_layernorm.weight', 'transformer.layers.9.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.22.post_attention_layernorm.weight', 'transformer.layers.16.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.9.input_layernorm.weight', 'transformer.layers.24.input_layernorm.weight', 'transformer.layers.31.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.25.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.6.input_layernorm.weight', 'prompt_encoder.transfromer.layers.8.mlp.up_proj.weight', 'transformer.layers.17.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.11.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.15.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.25.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.16.input_layernorm.weight', 'transformer.layers.18.self_attn.o_proj.weight', 'transformer.layers.25.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.11.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.1.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.17.post_attention_layernorm.weight', 'transformer.layers.29.self_attn.o_proj.weight', 'transformer.layers.31.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.k_proj.weight', 'transformer.layers.24.self_attn.q_proj.weight', 'transformer.layers.26.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.6.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.gate_proj.weight', 'transformer.layers.21.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.k_proj.weight', 'transformer.layers.10.mlp.down_proj.weight', 'transformer.layers.12.mlp.up_proj.weight', 'transformer.layers.10.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.19.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.24.self_attn.k_proj.weight', 'transformer.layers.23.self_attn.v_proj.weight', 'transformer.layers.30.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.11.input_layernorm.weight', 'prompt_encoder.transfromer.layers.20.input_layernorm.weight', 'transformer.layers.24.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.24.input_layernorm.weight', 'transformer.layers.14.mlp.gate_proj.weight', 'transformer.layers.26.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.19.self_attn.o_proj.weight', 'transformer.layers.4.self_attn.v_proj.weight', 'transformer.layers.13.post_attention_layernorm.weight', 'transformer.layers.17.mlp.up_proj.weight', 'transformer.layers.23.post_attention_layernorm.weight', 'transformer.layers.30.self_attn.o_proj.weight', 'transformer.layers.11.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.11.mlp.gate_proj.weight', 'transformer.layers.11.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.q_proj.weight', 'transformer.layers.9.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.v_proj.weight', 'transformer.layers.23.mlp.gate_proj.weight', 'transformer.layers.19.mlp.gate_proj.weight', 'transformer.layers.31.self_attn.q_proj.weight', 'transformer.layers.22.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.13.input_layernorm.weight', 'transformer.layers.19.input_layernorm.weight', 'transformer.layers.19.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.18.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.19.post_attention_layernorm.weight', 'transformer.layers.6.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.1.self_attn.v_proj.weight', 'transformer.layers.27.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.24.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.5.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.down_proj.weight', 'transformer.layers.13.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.o_proj.weight', 'transformer.layers.26.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.o_proj.weight', 'transformer.layers.26.post_attention_layernorm.weight', 'prompt_encoder.embedding.weight', 'prompt_encoder.transfromer.layers.6.self_attn.o_proj.weight', 'transformer.layers.22.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.4.input_layernorm.weight', 'transformer.layers.22.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.9.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.6.mlp.up_proj.weight', 'transformer.layers.3.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.o_proj.weight', 'transformer.layers.7.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.q_proj.weight', 'transformer.layers.7.input_layernorm.weight', 'prompt_encoder.transfromer.layers.31.self_attn.o_proj.weight', 'transformer.layers.5.input_layernorm.weight', 'transformer.layers.20.input_layernorm.weight', 'transformer.layers.25.input_layernorm.weight', 'prompt_encoder.transfromer.layers.8.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.20.mlp.gate_proj.weight', 'transformer.layers.0.mlp.gate_proj.weight', 'transformer.layers.21.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.8.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.o_proj.weight', 'transformer.layers.26.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.23.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.28.mlp.up_proj.weight', 'prompt_encoder.transfromer.norm.weight', 'transformer.layers.10.post_attention_layernorm.weight', 'transformer.layers.15.self_attn.v_proj.weight', 'transformer.layers.5.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.18.self_attn.q_proj.weight', 'transformer.layers.25.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.12.self_attn.o_proj.weight', 'transformer.layers.28.mlp.down_proj.weight', 'transformer.layers.31.self_attn.k_proj.weight', 'transformer.layers.14.post_attention_layernorm.weight', 'transformer.layers.4.mlp.down_proj.weight', 'transformer.layers.28.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.27.post_attention_layernorm.weight', 'transformer.layers.5.mlp.down_proj.weight', 'transformer.layers.27.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.22.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.k_proj.weight', 'transformer.layers.23.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.29.input_layernorm.weight', 'prompt_encoder.transfromer.layers.19.self_attn.k_proj.weight', 'transformer.layers.19.self_attn.o_proj.weight', 'transformer.layers.23.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.v_proj.weight', 'transformer.layers.1.mlp.down_proj.weight', 'transformer.layers.20.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.30.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.16.mlp.down_proj.weight', 'transformer.layers.10.self_attn.k_proj.weight', 'transformer.layers.27.self_attn.q_proj.weight', 'transformer.layers.30.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.12.input_layernorm.weight', 'transformer.layers.12.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.1.mlp.up_proj.weight', 'transformer.layers.6.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.18.input_layernorm.weight', 'prompt_encoder.transfromer.layers.25.mlp.down_proj.weight', 'transformer.layers.4.post_attention_layernorm.weight', 'transformer.layers.21.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.31.mlp.gate_proj.weight', 'transformer.layers.6.input_layernorm.weight', 'prompt_encoder.transfromer.layers.14.self_attn.q_proj.weight', 'transformer.layers.29.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.19.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.20.post_attention_layernorm.weight', 'transformer.layers.18.input_layernorm.weight', 'transformer.layers.27.post_attention_layernorm.weight', 'transformer.layers.5.mlp.gate_proj.weight', 'transformer.layers.29.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.2.mlp.gate_proj.weight', 'transformer.layers.3.self_attn.o_proj.weight', 'transformer.layers.3.post_attention_layernorm.weight', 'transformer.layers.2.mlp.down_proj.weight', 'transformer.layers.24.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.18.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.7.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.0.mlp.down_proj.weight', 'transformer.layers.17.self_attn.k_proj.weight', 'transformer.layers.15.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.14.self_attn.v_proj.weight', 'transformer.layers.12.self_attn.v_proj.weight', 'transformer.layers.16.self_attn.q_proj.weight', 'transformer.layers.2.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.1.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.4.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.q_proj.weight', 'transformer.layers.26.mlp.gate_proj.weight', 'transformer.layers.27.mlp.up_proj.weight', 'transformer.layers.11.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.28.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.v_proj.weight', 'transformer.embed_tokens.weight', 'transformer.layers.28.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.4.mlp.gate_proj.weight', 'transformer.layers.18.self_attn.k_proj.weight', 'transformer.layers.1.input_layernorm.weight', 'prompt_encoder.transfromer.layers.15.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.q_proj.weight', 'transformer.layers.18.mlp.gate_proj.weight', 'transformer.layers.13.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.gate_proj.weight', 'transformer.layers.14.input_layernorm.weight', 'transformer.layers.4.mlp.up_proj.weight', 'transformer.layers.0.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.10.post_attention_layernorm.weight', 'transformer.layers.30.mlp.down_proj.weight', 'transformer.layers.12.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.k_proj.weight', 'transformer.layers.22.self_attn.q_proj.weight', 'transformer.layers.23.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.9.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.v_proj.weight', 'transformer.layers.30.self_attn.q_proj.weight', 'transformer.layers.20.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.v_proj.weight', 'transformer.layers.29.self_attn.v_proj.weight', 'transformer.layers.9.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.22.input_layernorm.weight', 'prompt_encoder.transfromer.layers.6.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.30.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.15.mlp.down_proj.weight', 'transformer.layers.7.self_attn.q_proj.weight', 'transformer.layers.24.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.q_proj.weight', 'transformer.layers.22.mlp.gate_proj.weight', 'transformer.layers.16.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.0.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.4.post_attention_layernorm.weight', 'transformer.layers.8.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.23.mlp.down_proj.weight', 'transformer.layers.30.input_layernorm.weight', 'transformer.layers.20.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.3.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.o_proj.weight', 'transformer.layers.16.input_layernorm.weight', 'transformer.layers.16.mlp.gate_proj.weight', 'transformer.layers.11.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.gate_proj.weight', 'transformer.layers.14.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.down_proj.weight', 'transformer.layers.10.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.22.mlp.gate_proj.weight', 'transformer.layers.31.post_attention_layernorm.weight', 'transformer.layers.0.self_attn.k_proj.weight', 'transformer.layers.4.input_layernorm.weight', 'transformer.layers.8.self_attn.k_proj.weight', 'transformer.layers.9.input_layernorm.weight', 'prompt_encoder.transfromer.layers.5.self_attn.k_proj.weight', 'transformer.layers.8.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.5.mlp.gate_proj.weight', 'transformer.layers.22.input_layernorm.weight', 'transformer.layers.15.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.25.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.28.post_attention_layernorm.weight', 'transformer.layers.4.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.9.post_attention_layernorm.weight', 'transformer.layers.13.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.17.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.7.input_layernorm.weight', 'prompt_encoder.transfromer.layers.20.self_attn.q_proj.weight', 'transformer.layers.19.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.10.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.1.mlp.down_proj.weight', 'transformer.layers.18.self_attn.q_proj.weight', 'transformer.layers.1.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.up_proj.weight', 'transformer.layers.29.self_attn.q_proj.weight', 'transformer.layers.9.self_attn.v_proj.weight', 'transformer.layers.5.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.up_proj.weight', 'transformer.layers.2.self_attn.q_proj.weight', 'transformer.layers.14.self_attn.v_proj.weight', 'transformer.layers.14.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.23.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.19.input_layernorm.weight', 'transformer.layers.9.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.26.input_layernorm.weight', 'transformer.layers.30.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.12.post_attention_layernorm.weight', 'transformer.layers.6.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.1.input_layernorm.weight', 'transformer.layers.18.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.14.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.q_proj.weight', 'transformer.layers.9.mlp.up_proj.weight', 'transformer.layers.12.mlp.gate_proj.weight', 'transformer.layers.2.self_attn.o_proj.weight', 'transformer.layers.31.input_layernorm.weight', 'prompt_encoder.transfromer.layers.2.mlp.down_proj.weight', 'transformer.layers.22.post_attention_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#from falconSKT import  PrefixForTokenClassification\n",
    "\n",
    "model = PrefixForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "160ccc0b-127d-4616-9b96-6a1a2a5b65fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 7110729731\n",
      "Trainable Parameters: 69635\n",
      "Percentage Trainable: 0.00097929470862067290%\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters in the model\n",
    "total_parameters = model.num_parameters()\n",
    "\n",
    "# Total number of trainable parameters in the model\n",
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate the percentage of trainable parameters\n",
    "percentage_trainable = (trainable_parameters / total_parameters) * 100\n",
    "\n",
    "print(f\"Total Parameters: {total_parameters}\")\n",
    "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
    "print(f\"Percentage Trainable: {percentage_trainable:.20f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "758507d2-8e9b-4491-a970-385c1d374158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision = metrics.precision_score(labels, predictions, average=\"macro\")\n",
    "    recall = metrics.recall_score(labels, predictions, average=\"macro\")\n",
    "    f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c4f8aad-713e-421f-a8d5-581b802597b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2307' max='73250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2307/73250 1:20:07 < 41:06:15, 0.48 it/s, Epoch 0.06/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.421197</td>\n",
       "      <td>0.851192</td>\n",
       "      <td>0.846119</td>\n",
       "      <td>0.842998</td>\n",
       "      <td>0.847516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.386888</td>\n",
       "      <td>0.859350</td>\n",
       "      <td>0.851715</td>\n",
       "      <td>0.847399</td>\n",
       "      <td>0.853013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'prompt_encoder.transfromer.layers.16.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.5.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.8.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.22.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.17.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.12.input_layernorm.weight', 'prompt_encoder.transfromer.layers.21.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.1.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.3.input_layernorm.weight', 'prompt_encoder.transfromer.layers.18.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.18.input_layernorm.weight', 'prompt_encoder.transfromer.layers.4.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.25.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.22.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.31.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.16.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.14.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.9.input_layernorm.weight', 'prompt_encoder.transfromer.layers.28.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.19.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.20.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.25.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.6.input_layernorm.weight', 'prompt_encoder.transfromer.layers.8.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.11.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.15.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.2.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.27.input_layernorm.weight', 'prompt_encoder.transfromer.layers.25.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.16.input_layernorm.weight', 'prompt_encoder.transfromer.layers.23.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.11.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.10.input_layernorm.weight', 'prompt_encoder.transfromer.layers.1.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.9.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.15.input_layernorm.weight', 'prompt_encoder.transfromer.layers.4.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.17.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.18.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.24.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.7.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.24.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.0.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.11.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.21.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.23.input_layernorm.weight', 'prompt_encoder.transfromer.layers.9.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.14.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.31.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.29.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.6.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.26.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.0.input_layernorm.weight', 'prompt_encoder.transfromer.layers.0.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.2.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.27.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.12.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.14.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.19.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.0.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.1.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.4.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.11.input_layernorm.weight', 'prompt_encoder.transfromer.layers.20.input_layernorm.weight', 'prompt_encoder.transfromer.layers.24.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.21.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.24.input_layernorm.weight', 'prompt_encoder.transfromer.layers.28.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.23.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.7.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.v_proj.weight', 'prompt_encoder.transfromer.embed_tokens.weight', 'prompt_encoder.transfromer.layers.15.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.19.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.29.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.31.input_layernorm.weight', 'prompt_encoder.transfromer.layers.4.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.20.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.1.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.11.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.20.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.26.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.28.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.24.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.29.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.12.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.10.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.2.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.13.input_layernorm.weight', 'prompt_encoder.transfromer.layers.30.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.1.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.9.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.12.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.8.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.14.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.10.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.18.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.19.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.26.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.1.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.22.input_layernorm.weight', 'prompt_encoder.transfromer.layers.6.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.24.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.5.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.17.input_layernorm.weight', 'prompt_encoder.transfromer.layers.30.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.14.input_layernorm.weight', 'prompt_encoder.transfromer.layers.22.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.15.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.8.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.9.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.14.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.9.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.22.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.0.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.7.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.6.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.21.input_layernorm.weight', 'prompt_encoder.transfromer.layers.0.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.31.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.4.input_layernorm.weight', 'prompt_encoder.transfromer.layers.1.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.4.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.28.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.9.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.6.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.2.input_layernorm.weight', 'prompt_encoder.transfromer.layers.23.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.23.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.10.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.15.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.16.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.26.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.3.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.30.input_layernorm.weight', 'prompt_encoder.transfromer.layers.9.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.23.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.4.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.28.input_layernorm.weight', 'prompt_encoder.transfromer.layers.7.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.3.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.12.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.13.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.16.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.8.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.20.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.8.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.17.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.7.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.13.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.22.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.10.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.23.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.16.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.28.mlp.up_proj.weight', 'prompt_encoder.transfromer.norm.weight', 'prompt_encoder.transfromer.layers.18.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.5.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.5.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.0.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.7.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.3.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.28.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.4.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.12.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.25.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.28.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.30.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.8.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.16.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.17.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.9.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.29.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.7.input_layernorm.weight', 'prompt_encoder.transfromer.layers.20.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.gate_proj.weight', 'prompt_encoder.transfromer.layers.10.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.1.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.27.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.13.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.2.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.8.input_layernorm.weight', 'prompt_encoder.transfromer.layers.27.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.0.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.19.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.29.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.24.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.6.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.18.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.25.input_layernorm.weight', 'prompt_encoder.transfromer.layers.23.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.17.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.22.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.11.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.31.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.20.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.29.input_layernorm.weight', 'prompt_encoder.transfromer.layers.15.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.19.input_layernorm.weight', 'prompt_encoder.transfromer.layers.5.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.19.self_attn.k_proj.weight', 'prompt_encoder.transfromer.layers.21.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.26.input_layernorm.weight', 'prompt_encoder.transfromer.layers.12.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.30.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.1.input_layernorm.weight', 'prompt_encoder.transfromer.layers.14.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.18.post_attention_layernorm.weight', 'prompt_encoder.transfromer.layers.5.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.25.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.6.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.15.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.30.self_attn.o_proj.weight', 'prompt_encoder.transfromer.layers.2.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.5.input_layernorm.weight', 'prompt_encoder.transfromer.layers.21.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.23.self_attn.v_proj.weight', 'prompt_encoder.transfromer.layers.27.mlp.up_proj.weight', 'prompt_encoder.transfromer.layers.31.self_attn.q_proj.weight', 'prompt_encoder.transfromer.layers.2.mlp.down_proj.weight', 'prompt_encoder.transfromer.layers.29.self_attn.v_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 47.54 GiB total capacity; 45.94 GiB already allocated; 19.12 MiB free; 46.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./r_task\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#learning_rate=1e-5,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m, in \u001b[0;36mPrefixForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    154\u001b[0m prompt_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, pre_length)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((prompt_attention_mask, attention_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:938\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    929\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    930\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m         use_cache,\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:676\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    675\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 676\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    679\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:177\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 47.54 GiB total capacity; 45.94 GiB already allocated; 19.12 MiB free; 46.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./r_task',\n",
    "    #learning_rate=1e-5,\n",
    "    per_device_train_batch_size=15,\n",
    "    per_device_eval_batch_size=15,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8505e-816a-4b36-95db-bd8eee123cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9729f32-3aa4-44dd-97bd-b66e23c0681f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
